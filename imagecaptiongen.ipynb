{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:13.774218Z",
     "iopub.status.busy": "2022-11-07T09:58:13.773878Z",
     "iopub.status.idle": "2022-11-07T09:58:13.802179Z",
     "shell.execute_reply": "2022-11-07T09:58:13.801305Z",
     "shell.execute_reply.started": "2022-11-07T09:58:13.774133Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, warnings, random, string\n",
    "from pickle import dump, load\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:13.804566Z",
     "iopub.status.busy": "2022-11-07T09:58:13.804229Z",
     "iopub.status.idle": "2022-11-07T09:58:19.886263Z",
     "shell.execute_reply": "2022-11-07T09:58:19.885295Z",
     "shell.execute_reply.started": "2022-11-07T09:58:13.804533Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from keras.applications.inception_v3 import InceptionV3 , preprocess_input\n",
    "from keras.layers import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:19.888341Z",
     "iopub.status.busy": "2022-11-07T09:58:19.887618Z",
     "iopub.status.idle": "2022-11-07T09:58:20.725629Z",
     "shell.execute_reply": "2022-11-07T09:58:20.724627Z",
     "shell.execute_reply.started": "2022-11-07T09:58:19.888299Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset and Performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:20.729690Z",
     "iopub.status.busy": "2022-11-07T09:58:20.729074Z",
     "iopub.status.idle": "2022-11-07T09:58:20.734660Z",
     "shell.execute_reply": "2022-11-07T09:58:20.733267Z",
     "shell.execute_reply.started": "2022-11-07T09:58:20.729649Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dir = '../input/flickr8k/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:20.736661Z",
     "iopub.status.busy": "2022-11-07T09:58:20.736183Z",
     "iopub.status.idle": "2022-11-07T09:58:20.855542Z",
     "shell.execute_reply": "2022-11-07T09:58:20.854567Z",
     "shell.execute_reply.started": "2022-11-07T09:58:20.736618Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_captions_dictionary(path):\n",
    "    file = open(path, 'r')\n",
    "    captions = file.read().split('\\n')\n",
    "    descriptions = {}\n",
    "    for text in captions[1:]:\n",
    "        values = text.split(',')\n",
    "        img, caption = values[0].split('.')[0], \"\".join(values[1:])\n",
    "        if img not in descriptions:\n",
    "            descriptions[img] = [caption]\n",
    "        else:\n",
    "            descriptions[img].append(caption)\n",
    "    file.close()\n",
    "    return descriptions\n",
    "\n",
    "descriptions = load_captions_dictionary(input_dir + 'captions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:20.857465Z",
     "iopub.status.busy": "2022-11-07T09:58:20.856996Z",
     "iopub.status.idle": "2022-11-07T09:58:24.150958Z",
     "shell.execute_reply": "2022-11-07T09:58:24.150119Z",
     "shell.execute_reply.started": "2022-11-07T09:58:20.857425Z"
    }
   },
   "outputs": [],
   "source": [
    "npic = 5\n",
    "img_size = 299\n",
    "target_size = (img_size, img_size)\n",
    "path = input_dir + \"Images/\"\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "\n",
    "count = 1\n",
    "for img in os.listdir(path)[:npic]:\n",
    "    \n",
    "    filename = path + img\n",
    "    captions = list(descriptions[img.split(\".\")[0]])\n",
    "    image_load = load_img(filename, target_size=target_size)\n",
    "    \n",
    "    ax = fig.add_subplot(npic, 2, count, xticks=[], yticks=[])\n",
    "    ax.imshow(image_load)\n",
    "    count += 1\n",
    "    \n",
    "    ax = fig.add_subplot(npic, 2, count)\n",
    "    plt.axis('off')\n",
    "    ax.plot()\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, len(captions))\n",
    "    for i, caption in enumerate(captions):\n",
    "        ax.text(0, i, caption, fontsize=20)\n",
    "    count += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:24.152236Z",
     "iopub.status.busy": "2022-11-07T09:58:24.151886Z",
     "iopub.status.idle": "2022-11-07T09:58:24.589722Z",
     "shell.execute_reply": "2022-11-07T09:58:24.588796Z",
     "shell.execute_reply.started": "2022-11-07T09:58:24.152193Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaning(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for img, caption in descriptions.items():\n",
    "        for i, img_text in enumerate(caption):\n",
    "            img_text.replace(\"-\", \" \")\n",
    "            text = [word.lower() for word in img_text.split()]\n",
    "            text = [word.translate(table) for word in text]\n",
    "            text = [word for word in text if(len(word) > 1)]\n",
    "            text = [word for word in text if(word.isalpha())]\n",
    "            img_text = \" \".join(text)\n",
    "            descriptions[img][i] = img_text\n",
    "    return descriptions\n",
    "\n",
    "descriptions = text_cleaning(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:24.591629Z",
     "iopub.status.busy": "2022-11-07T09:58:24.591259Z",
     "iopub.status.idle": "2022-11-07T09:58:24.656625Z",
     "shell.execute_reply": "2022-11-07T09:58:24.655559Z",
     "shell.execute_reply.started": "2022-11-07T09:58:24.591593Z"
    }
   },
   "outputs": [],
   "source": [
    "def corpus_and_vocab(descriptions):\n",
    "    corpus = \"\"\n",
    "    for img_text in descriptions.values():\n",
    "        for text in img_text:\n",
    "            corpus += \" \"+text\n",
    "    vocab = set(corpus.split())\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = corpus_and_vocab(descriptions)\n",
    "print(\"Number of unique words = {}\".format(len(vocab)))\n",
    "# Number of unique words being displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:24.658688Z",
     "iopub.status.busy": "2022-11-07T09:58:24.658326Z",
     "iopub.status.idle": "2022-11-07T09:58:24.927018Z",
     "shell.execute_reply": "2022-11-07T09:58:24.926089Z",
     "shell.execute_reply.started": "2022-11-07T09:58:24.658652Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(corpus.split())\n",
    "dfsub = pd.DataFrame(columns = [\"word\", \"count\"])\n",
    "most_common = freq_dist.most_common()\n",
    "words, counts = [], []\n",
    "for i in range(len(freq_dist)):\n",
    "    words.append(most_common[i][0])\n",
    "    counts.append(most_common[i][1])\n",
    "dfsub[\"word\"], dfsub[\"count\"] = words, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:24.931940Z",
     "iopub.status.busy": "2022-11-07T09:58:24.931653Z",
     "iopub.status.idle": "2022-11-07T09:58:26.202903Z",
     "shell.execute_reply": "2022-11-07T09:58:26.201926Z",
     "shell.execute_reply.started": "2022-11-07T09:58:24.931914Z"
    }
   },
   "outputs": [],
   "source": [
    "def plthist(dfsub, title):\n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.bar(dfsub.index,dfsub[\"count\"])\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "plthist(dfsub.iloc[:50], \"The top 50 most frequently appearing words\")\n",
    "plthist(dfsub.iloc[-50:], \"The least 50 most frequently appearing words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:26.204680Z",
     "iopub.status.busy": "2022-11-07T09:58:26.204242Z",
     "iopub.status.idle": "2022-11-07T09:58:26.244332Z",
     "shell.execute_reply": "2022-11-07T09:58:26.243246Z",
     "shell.execute_reply.started": "2022-11-07T09:58:26.204641Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i, desc in enumerate(desc_list):\n",
    "            descriptions[key][i] = desc = \"<start> \" + desc + \" <end>\"\n",
    "            lines.append(key + '\\t' + desc)\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename,\"w\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "save_descriptions(descriptions, \"Image_Descriptions_List.txt\")\n",
    "# Putting start and end tags at the beginning and ending of each caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_Test_Val Split\n",
    "#### First 10 images and captions are for the Test set\n",
    "#### The next 10 are for the Validation set\n",
    "#### All of the remaining images and captions are for the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:26.248218Z",
     "iopub.status.busy": "2022-11-07T09:58:26.247885Z",
     "iopub.status.idle": "2022-11-07T09:58:26.279398Z",
     "shell.execute_reply": "2022-11-07T09:58:26.278404Z",
     "shell.execute_reply.started": "2022-11-07T09:58:26.248191Z"
    }
   },
   "outputs": [],
   "source": [
    "df_img_caption = pd.DataFrame()\n",
    "df_img_caption['Image_Name'] = list(descriptions.keys())[:-1]\n",
    "temps = list(descriptions.values())[:-1]\n",
    "df_img_caption['Caption'] = [temps[i][random.randint(0,4)] for i in range(len(temps))]\n",
    "df_img_caption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:26.281481Z",
     "iopub.status.busy": "2022-11-07T09:58:26.281124Z",
     "iopub.status.idle": "2022-11-07T09:58:26.288666Z",
     "shell.execute_reply": "2022-11-07T09:58:26.287759Z",
     "shell.execute_reply.started": "2022-11-07T09:58:26.281446Z"
    }
   },
   "outputs": [],
   "source": [
    "test_images = np.asarray(df_img_caption['Image_Name'][:10], dtype = np.dtype(object))\n",
    "test_captions = np.asarray(df_img_caption['Caption'][:10], dtype = np.dtype(object))\n",
    "val_images = np.asarray(df_img_caption['Image_Name'][10:20], dtype = np.dtype(object))\n",
    "val_captions = np.asarray(df_img_caption['Caption'][10:20], dtype = np.dtype(object))\n",
    "train_images = np.asarray(df_img_caption['Image_Name'][20:], dtype = np.dtype(object))\n",
    "train_captions = np.asarray(df_img_caption['Caption'][20:], dtype = np.dtype(object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "#### Using the InceptionV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:26.291011Z",
     "iopub.status.busy": "2022-11-07T09:58:26.289978Z",
     "iopub.status.idle": "2022-11-07T09:58:31.894508Z",
     "shell.execute_reply": "2022-11-07T09:58:31.893508Z",
     "shell.execute_reply.started": "2022-11-07T09:58:26.290864Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_model = InceptionV3(weights = 'imagenet')\n",
    "for layer in cnn_model.layers:\n",
    "    layer.trainable = False\n",
    "cnn_model = Model(inputs = cnn_model.input, outputs = cnn_model.get_layer('avg_pool').output)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:31.896571Z",
     "iopub.status.busy": "2022-11-07T09:58:31.895819Z",
     "iopub.status.idle": "2022-11-07T09:58:31.904331Z",
     "shell.execute_reply": "2022-11-07T09:58:31.902986Z",
     "shell.execute_reply.started": "2022-11-07T09:58:31.896529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to extract features\n",
    "def extract_features(model, images, img_size):\n",
    "    features = {}\n",
    "    for img in tqdm(images):\n",
    "        picture = load_img(input_dir + \"Images/\" + img + \".jpg\", target_size = (img_size, img_size))\n",
    "        picture = img_to_array(picture)\n",
    "        picture = np.expand_dims(picture, axis = 0)\n",
    "        picture = preprocess_input(picture)\n",
    "        features[img] = model.predict(picture).reshape(2048,)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T09:58:31.906667Z",
     "iopub.status.busy": "2022-11-07T09:58:31.905655Z",
     "iopub.status.idle": "2022-11-07T10:07:50.915881Z",
     "shell.execute_reply": "2022-11-07T10:07:50.914875Z",
     "shell.execute_reply.started": "2022-11-07T09:58:31.906632Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training set features\n",
    "Xtrain_features = extract_features(cnn_model, train_images, 299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:07:50.918050Z",
     "iopub.status.busy": "2022-11-07T10:07:50.917188Z",
     "iopub.status.idle": "2022-11-07T10:07:51.686541Z",
     "shell.execute_reply": "2022-11-07T10:07:51.685566Z",
     "shell.execute_reply.started": "2022-11-07T10:07:50.917996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validation set features\n",
    "Xval_features = extract_features(cnn_model, val_images, 299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:07:51.688465Z",
     "iopub.status.busy": "2022-11-07T10:07:51.688088Z",
     "iopub.status.idle": "2022-11-07T10:07:51.727045Z",
     "shell.execute_reply": "2022-11-07T10:07:51.726091Z",
     "shell.execute_reply.started": "2022-11-07T10:07:51.688428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save features as an array\n",
    "Xtrain_features = np.asarray(list(Xtrain_features.values()))\n",
    "Xval_features = np.asarray(list(Xval_features.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:07:51.729825Z",
     "iopub.status.busy": "2022-11-07T10:07:51.729451Z",
     "iopub.status.idle": "2022-11-07T10:07:51.866504Z",
     "shell.execute_reply": "2022-11-07T10:07:51.865583Z",
     "shell.execute_reply.started": "2022-11-07T10:07:51.729788Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split up captions into smaller parts\n",
    "tokenizer = Tokenizer(num_words = len(vocab))\n",
    "tokenizer.fit_on_texts(df_img_caption['Caption'])\n",
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = dict([index, word] for word, index in word_to_index.items())\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:07:51.868709Z",
     "iopub.status.busy": "2022-11-07T10:07:51.867752Z",
     "iopub.status.idle": "2022-11-07T10:07:51.976957Z",
     "shell.execute_reply": "2022-11-07T10:07:51.976152Z",
     "shell.execute_reply.started": "2022-11-07T10:07:51.868672Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_captions)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_captions)\n",
    "\n",
    "def maxLength(sequences):\n",
    "    return np.max([len(sequence) for sequence in sequences])\n",
    "\n",
    "max_len = max(maxLength(train_sequences), maxLength(val_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:07:51.978871Z",
     "iopub.status.busy": "2022-11-07T10:07:51.978188Z",
     "iopub.status.idle": "2022-11-07T10:08:02.155171Z",
     "shell.execute_reply": "2022-11-07T10:08:02.153975Z",
     "shell.execute_reply.started": "2022-11-07T10:07:51.978835Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define Function for data to be generated so as to be fed into the rnn model\n",
    "def data_generator(features, sequences):\n",
    "    X_features, X_train, y_train = [], [], []\n",
    "    for sequence, feature in zip(sequences, features):\n",
    "        for i in range(1, len(sequence)):\n",
    "            in_text, out_text = sequence[:i], sequence[i:]\n",
    "            in_text = pad_sequences([in_text], maxlen = max_len)[0]\n",
    "            out_text = to_categorical(out_text, num_classes = vocab_size)[0]\n",
    "            X_features.append(feature)\n",
    "            X_train.append(in_text)\n",
    "            y_train.append(out_text)\n",
    "    return (np.array(X_features), np.array(X_train), np.array(y_train))\n",
    "\n",
    "Xt_features, Xt_text, yt_text = data_generator(Xtrain_features, train_sequences)\n",
    "Xv_features, Xv_text, yv_text = data_generator(Xval_features, val_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:08:02.157538Z",
     "iopub.status.busy": "2022-11-07T10:08:02.156765Z",
     "iopub.status.idle": "2022-11-07T10:08:02.163240Z",
     "shell.execute_reply": "2022-11-07T10:08:02.162097Z",
     "shell.execute_reply.started": "2022-11-07T10:08:02.157502Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Xt_features.shape, Xt_text.shape, yt_text.shape)\n",
    "print(Xv_features.shape, Xv_text.shape, yv_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:08:02.165569Z",
     "iopub.status.busy": "2022-11-07T10:08:02.164899Z",
     "iopub.status.idle": "2022-11-07T10:08:04.204285Z",
     "shell.execute_reply": "2022-11-07T10:08:04.202829Z",
     "shell.execute_reply.started": "2022-11-07T10:08:02.165534Z"
    }
   },
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_len):\n",
    "\n",
    "    inputs1 = Input(shape = (2048,))\n",
    "    x1 = Dropout(0.3)(inputs1)\n",
    "    x2 = Dense(256, activation = 'relu')(x1)\n",
    "\n",
    "    inputs2 = Input(shape = (max_len,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero = True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    decoder1 = add([x2, se3])\n",
    "    decoder2 = Dense(256, activation = 'relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "\n",
    "    rnn_model = Model(inputs = [inputs1, inputs2], outputs = outputs)\n",
    "    rnn_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "    print(rnn_model.summary())\n",
    "    plot_model(rnn_model, to_file = 'rnn_model.png', show_shapes = True)\n",
    "\n",
    "    return rnn_model\n",
    "\n",
    "rnn_model = define_model(vocab_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:08:04.206962Z",
     "iopub.status.busy": "2022-11-07T10:08:04.206563Z",
     "iopub.status.idle": "2022-11-07T10:18:35.546203Z",
     "shell.execute_reply": "2022-11-07T10:18:35.545246Z",
     "shell.execute_reply.started": "2022-11-07T10:08:04.206918Z"
    }
   },
   "outputs": [],
   "source": [
    "model_training = rnn_model.fit([Xt_features, Xt_text], yt_text, \n",
    "                               epochs = 5, verbose = 2, batch_size = 64, \n",
    "                               validation_data = ([Xv_features, Xv_text], yv_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:18:35.548786Z",
     "iopub.status.busy": "2022-11-07T10:18:35.547704Z",
     "iopub.status.idle": "2022-11-07T10:18:35.772022Z",
     "shell.execute_reply": "2022-11-07T10:18:35.770968Z",
     "shell.execute_reply.started": "2022-11-07T10:18:35.548746Z"
    }
   },
   "outputs": [],
   "source": [
    "for label in [\"loss\", \"val_loss\"]:\n",
    "    plt.plot(model_training.history[label], label = label)\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:18:35.773861Z",
     "iopub.status.busy": "2022-11-07T10:18:35.773521Z",
     "iopub.status.idle": "2022-11-07T10:18:36.376957Z",
     "shell.execute_reply": "2022-11-07T10:18:36.375995Z",
     "shell.execute_reply.started": "2022-11-07T10:18:35.773826Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving models\n",
    "rnn_model.save(\"RNN_Model.h5\")\n",
    "cnn_model.save(\"CNN_Model.h5\")\n",
    "dump(tokenizer, open('Flickr8K_Tokenizer.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:18:36.378788Z",
     "iopub.status.busy": "2022-11-07T10:18:36.378450Z",
     "iopub.status.idle": "2022-11-07T10:18:36.386986Z",
     "shell.execute_reply": "2022-11-07T10:18:36.386012Z",
     "shell.execute_reply.started": "2022-11-07T10:18:36.378749Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_caption(filename):\n",
    "    img = load_img(filename, target_size = (299, 299))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    img = preprocess_input(img)\n",
    "    features = cnn_model.predict(img)\n",
    "    in_text = 'start'\n",
    "    for i in range(max_len):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_len)\n",
    "        pred = rnn_model.predict([features,sequence], verbose=0)\n",
    "        pred = np.argmax(pred)\n",
    "        word = index_to_word[pred]\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'end':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:18:36.392961Z",
     "iopub.status.busy": "2022-11-07T10:18:36.392655Z",
     "iopub.status.idle": "2022-11-07T10:18:36.403517Z",
     "shell.execute_reply": "2022-11-07T10:18:36.402591Z",
     "shell.execute_reply.started": "2022-11-07T10:18:36.392936Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_caption_beam_search(filename, max_length, beam_index):\n",
    "    img = load_img(filename, target_size = (299, 299))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    img = preprocess_input(img)\n",
    "    features = cnn_model.predict(img)\n",
    "    in_text = [[tokenizer.texts_to_sequences(['start'])[0], 0.0]]\n",
    "    while len(in_text[0][0]) < max_length:\n",
    "        tempList = []\n",
    "        for seq in in_text:\n",
    "            padded_seq = pad_sequences([seq[0]], maxlen=max_length)\n",
    "            preds = rnn_model.predict([features,padded_seq], verbose=0)\n",
    "            top_preds = np.argsort(preds[0])[-beam_index:]\n",
    "            for word in top_preds:\n",
    "                next_seq, prob = seq[0][:], seq[1]\n",
    "                next_seq.append(word)\n",
    "                prob += preds[0][word]\n",
    "                tempList.append([next_seq, prob])\n",
    "        in_text = tempList\n",
    "        in_text = sorted(in_text, reverse=False, key=lambda l: l[1])\n",
    "        in_text = in_text[-beam_index:]\n",
    "    in_text = in_text[-1][0]\n",
    "    final_caption_raw = [index_to_word[i] for i in in_text]\n",
    "    final_caption = []\n",
    "    for word in final_caption_raw:\n",
    "        if word == 'end':\n",
    "            break\n",
    "        else:\n",
    "            final_caption.append(word)\n",
    "    final_caption.append('end')\n",
    "    return ' '.join(final_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:18:36.405481Z",
     "iopub.status.busy": "2022-11-07T10:18:36.404650Z",
     "iopub.status.idle": "2022-11-07T10:19:53.868839Z",
     "shell.execute_reply": "2022-11-07T10:19:53.867902Z",
     "shell.execute_reply.started": "2022-11-07T10:18:36.405442Z"
    }
   },
   "outputs": [],
   "source": [
    "scores, beam2_scores, beam3_scores = [], [], []\n",
    "for img, caption in zip(test_images, test_captions):\n",
    "    hypothesis = generate_caption(input_dir + \"Images/\" + img + \".jpg\")\n",
    "    scores.append(sentence_bleu([caption.split()], hypothesis.split()))\n",
    "    hypothesis = generate_caption_beam_search(input_dir + \"Images/\" + img + \".jpg\", max_len, 2)\n",
    "    beam2_scores.append(sentence_bleu([caption.split()], hypothesis.split()))\n",
    "    hypothesis = generate_caption_beam_search(input_dir + \"Images/\" + img + \".jpg\", max_len, 3)\n",
    "    beam3_scores.append(sentence_bleu([caption.split()], hypothesis.split()))\n",
    "    \n",
    "for i, score in enumerate([scores, beam2_scores, beam3_scores]):\n",
    "    plt.plot(score, label = 'Beam Length = ' + str(i+1))\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlabel('Images')\n",
    "plt.ylabel('Bleu Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:19:53.871310Z",
     "iopub.status.busy": "2022-11-07T10:19:53.870279Z",
     "iopub.status.idle": "2022-11-07T10:19:59.507862Z",
     "shell.execute_reply": "2022-11-07T10:19:59.507047Z",
     "shell.execute_reply.started": "2022-11-07T10:19:53.871273Z"
    }
   },
   "outputs": [],
   "source": [
    "npic = 10\n",
    "npix = 299\n",
    "target_size = (npix,npix,3)\n",
    "\n",
    "count = 1\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for img, true_caption in zip(test_images, test_captions):\n",
    "     \n",
    "    filename = input_dir + 'Images/' + img + \".jpg\"\n",
    "    image_load = load_img(filename, target_size = target_size)\n",
    "    ax = fig.add_subplot(npic, 2, count, xticks=[], yticks=[])\n",
    "    true_caption = ' '.join(true_caption.split()[1: -1])\n",
    "    ax.imshow(image_load)\n",
    "    count += 1\n",
    "\n",
    "    caption = generate_caption(filename)\n",
    "    caption = ' '.join(caption.split()[1: -1])\n",
    "    ax = fig.add_subplot(npic, 2, count)\n",
    "    plt.axis('off')\n",
    "    ax.plot()\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.text(0, 0.7, true_caption, fontsize = 20)\n",
    "    ax.text(0, 0.4, caption, fontsize = 20)\n",
    "    ax.text(0, 0.1, 'Bleu Score = {}'.format(sentence_bleu([true_caption.split()], caption.split())), fontsize = 20)\n",
    "    count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T10:19:59.509929Z",
     "iopub.status.busy": "2022-11-07T10:19:59.509156Z",
     "iopub.status.idle": "2022-11-07T10:19:59.864916Z",
     "shell.execute_reply": "2022-11-07T10:19:59.863935Z",
     "shell.execute_reply.started": "2022-11-07T10:19:59.509882Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn_model.save_weights('model.h5')\n",
    "cnn_model.save_weights('model.h51')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
